'use server';
/**
 * generate-response.ts
 *
 * This file defines the core AI flow for the Gemini Sandbox application.
 * It is responsible for generating a response from the model based on user input,
 * conversation memory, and available tools.
 *
 * Key features:
 * - **Tool-Aware:** It can request the execution of predefined tools.
 * - **Memory-Aware:** It takes into account facts from the current conversation.
 * - **Step-by-Step Execution:** This flow executes ONE step at a time.
 *   If the model requests a tool, the flow returns the `toolRequest` to the calling action. The action
 *   is then responsible for executing the tool and calling this flow again with the `toolResponse`.
 *
 * This design allows the controlling server action to have full visibility and log each step of the agent's
 * reasoning and tool-use process.
 */

import { ai } from '@/ai/genkit';
import { z } from 'genkit';
import { mathEvaluator, todoTool, conversationReviewTool } from '@/mcp/tools';

// -----------------------------
// Schemas
// -----------------------------

const ToolRequestSchema = z.object({
  name: z.string().describe('Tool name to call'),
  input: z.any().optional().describe('Tool input payload'),
});

const GenerateResponseInputSchema = z.object({
  userMessage: z.string().describe('The user message to be processed.'),
  memory: z
    .array(z.string())
    .describe('A list of facts the agent has remembered from the conversation.'),
  toolResponse: z
    .any()
    .optional()
    .describe('Optional result from a tool run (for tool loops)'),
  previousReasoning: z
    .string()
    .optional()
    .describe(
      'The reasoning from the previous turn, provided when a tool was called.'
    ),
  userId: z.string().optional().describe('The ID of the current user.'),
  sessionId: z.string().optional().describe('The ID of the current session.'),
});
export type GenerateResponseInput = z.infer<typeof GenerateResponseInputSchema>;

const GenerateResponseOutputSchema = z.object({
  response: z.string().optional().describe('The response generated by the model.'),
  reasoning: z
    .string()
    .optional()
    .describe('The step-by-step reasoning process of the model.'),
  newFacts: z
    .array(z.string())
    .optional()
    .describe('New facts to save to memory.'),
  toolRequest: ToolRequestSchema.optional().describe(
    'Optional suggested tool request (model asks the runtime to run a tool)'
  ),
});
export type GenerateResponseOutput = z.infer<
  typeof GenerateResponseOutputSchema
>;

// -----------------------------
// Prompt
// -----------------------------

const generateResponsePrompt = ai.definePrompt({
  name: 'generateResponseWithToolsPrompt',
  tools: [mathEvaluator, todoTool, conversationReviewTool],
  input: { schema: GenerateResponseInputSchema },
  output: { schema: GenerateResponseOutputSchema },
  prompt: `You are a helpful assistant with memory and access to tools.

You are given:
- A list of memory facts (use them if relevant).
- The user's message.
- The user's ID (userId) and the current session ID (sessionId), which you MUST pass to any tools that require them (like conversationReviewTool or todoTool).
- Optionally, the result of a previously-run tool in 'toolResponse'.
- Optionally, your reasoning from the previous step in 'previousReasoning'.

Your goals:
1. Answer the user's message directly and conversationally.
2. If you need external actions or data (math, todo operations, searching the conversation), request a tool by returning a structured object in 'toolRequest'.
   e.g., { "toolRequest": { "name": "mathEvaluator", "input": { "expression": "2+2" } } }
   e.g., { "toolRequest": { "name": "conversationReviewTool", "input": { "query": "what I said about tasks", "userId": "{{userId}}", "sessionId": "{{sessionId}}" } } }

   If you request a tool, **do not** also provide the final user-facing \`response\`. The runtime will execute the tool and feed the result back to you. After receiving the \`toolResponse\`, you can then formulate the final answer.

3. Provide \`reasoning\` (brief) explaining how you answered or why you requested the tool.
4. Return \`newFacts\` â€” small, useful facts to save to memory (or an empty array).

Remember:
- If \`toolResponse\` is present, use it along with your 'previousReasoning' to understand the context. Then, formulate and provide a final user-facing \`response\`.
- Only request tools when absolutely necessary. If the user is just chatting, have a normal conversation.

Current Memory/Facts:
{{#if memory}}
{{#each memory}}
- {{{this}}}
{{/each}}
{{else}}
- Memory is empty.
{{/if}}

User message:
{{{userMessage}}}

{{#if previousReasoning}}
Your previous reasoning:
{{{previousReasoning}}}
{{/if}}

Previous tool result (if any):
{{#if toolResponse}}
{{{toolResponse}}}
{{else}}
- none
{{/if}}
`,
});

// -----------------------------
// Flow (executes a single step)
// -----------------------------
export async function generateResponse(
  input: GenerateResponseInput
): Promise<GenerateResponseOutput> {
  const { output } = await generateResponsePrompt(input);
  if (!output) {
    throw new Error('No output from model');
  }
  return output;
}
